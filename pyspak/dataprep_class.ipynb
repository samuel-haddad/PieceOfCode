{"cells":[{"cell_type":"code","source":["# libs\nfrom pyspark.sql import SparkSession\n\n# create session\nspark = SparkSession.builder\\\n        .master(\"local\")\\\n        .appName(\"Colab\")\\\n        .config('spark.ui.port', '4050')\\\n        .getOrCreate()\n\nclass DataPrep:\n    def __init__(self,dataframe_dict):\n        self.dataframe_dict=dataframe_dict\n    \n    \n    #--------------------------------------# CREATE AND STORAGE #--------------------------------------#\n    \n    def storage_df(self):\n        \n        \"\"\"\n        Creates and stores a dataframe.\n\n        Returns:\n            df (pyspark.sql.DataFrame): The persisted dataframe.\n        \"\"\"\n        \n        from pyspark import StorageLevel\n        \n        dataframe_dict = self.dataframe_dict\n\n        # create dataframe\n        if isinstance(dataframe_dict['schema'], str):\n            df = spark.table(dataframe_dict['schema'])\n            df = df.persist(StorageLevel.DISK_ONLY)\n        else:\n            df = dataframe_dict['schema']\n\n        return df\n    \n    \n    #--------------------------------------# PREP: ARRAY STRING TO STRUCT #--------------------------------------#\n    \n    def json_string_column_to_struct(self):\n        \n        from pyspark.sql.functions import from_json\n        \n        # create objects\n        dataframe=self.storage_df()\n        json_list=self.dataframe_dict['json_list']\n        \n        # transform\n        for column in json_list:\n            json_schema = spark.read.json(dataframe.rdd.map(lambda row: row[str(column)])).schema\n            dataframe = dataframe.withColumn(f'{column}_struct', from_json(str(column), json_schema))\n            dataframe = dataframe.drop(column).withColumnRenamed(f'{column}_struct', f'{column}')\n        \n        return dataframe\n    \n    \n    #--------------------------------------# SELECT AND RENAME #--------------------------------------#\n    \n    def basic_prep(self):\n        \n        from pyspark.sql.functions import col\n        \n        # create objects\n        dataframe_dict = self.dataframe_dict\n        dataframe = self.json_string_column_to_struct()\n        \n        # transform\n        df = (\n            dataframe\n            .select(*dataframe_dict['columns'].keys())\n            .toDF(*dataframe_dict['columns'].values())\n        )\n        \n        for col_name in dataframe_dict['pk_list']:\n            df = df.filter(col(col_name).isNotNull())\n            \n        return df\n    \n    #--------------------------------------# COMPOSED KEYS #--------------------------------------#\n    \n    def composed_keys(self):\n        \n        from pyspark.sql.functions import col, concat_ws, lower\n\n        # create objects\n        dataframe_dict = self.dataframe_dict\n        key=dataframe_dict['key'] \n        pk_name=dataframe_dict['pk_name']\n        pk_list=dataframe_dict['pk_list']\n        fk_name=dataframe_dict['fk_name']\n        fk_list=dataframe_dict['fk_list']\n        dataframe = self.basic_prep()\n\n        # pk transform\n        df = (\n            dataframe\n            .withColumn(\n                f'{pk_name}',\n                concat_ws('_', col(str(pk_list[0])), col(str(pk_list[1])))\n            )\n        )\n        \n        # fk transform\n        for name, list_cols in zip(fk_name, fk_list):\n            df = (\n                df\n                .withColumn(\n                    f'{name}',\n                    concat_ws('_', lower(col(str(list_cols[0]))), lower(col(str(list_cols[1]))))\n                )\n            )\n        \n        return df\n    \n    \n    #--------------------------------------# CREATE FLAGS #--------------------------------------#\n    \n    def flag_function(self):\n        from pyspark.sql.functions import col, when\n\n        # create objects\n        dataframe_dict = self.dataframe_dict\n        drop_list=dataframe_dict['drop_list']\n        df = self.composed_keys()\n        \n        for column in dataframe_dict['create_flag']:\n            df = df.withColumn(f'flag_{column}', when((col(column).isNotNull()) & (col(column) != 0), 1).otherwise(0))\n        \n        df = df.drop(*drop_list)\n\n        return df\n    \n    \n    #--------------------------------------# GROUP BY: FOREIGN KEYS #--------------------------------------#\n    \n    def groupby_function(self):\n        \n        # create objects\n        dataframe_dict = self.dataframe_dict\n        dataframe = self.flag_function()\n    \n        if dataframe_dict['key'] == 'fk':\n            df = (\n                dataframe\n                .groupBy(dataframe_dict['fk_name']).max()\n            )\n\n            for column in [column for column in dataframe.columns if column != dataframe_dict['fk_name']]:\n                df = df.withColumnRenamed(f\"max({column})\", f\"{column}\") \n                \n        elif dataframe_dict['key'] == 'pk':\n            df = self.flag_function()\n\n        return df\n    \n    \n    #--------------------------------------# RANKING: FOREIGN KEYS #--------------------------------------#\n    \n    def ranking(self):\n        from pyspark.sql.functions import col, lower, row_number\n        from pyspark.sql.window import Window\n        \n        # create objects\n        dataframe_dict = self.dataframe_dict\n        dataframe = self.groupby_function()\n        \n        tmp_dict = {}\n\n        if dataframe_dict['ranking'] == True:\n            # ranking\n            df = (\n                dataframe\n                .select(dataframe_dict['partition'], dataframe_dict['col_rank'])\n                .withColumn('rank', row_number().over(Window.partitionBy(dataframe_dict['partition']).orderBy(col(dataframe_dict['col_rank']).desc())))\n                .groupBy(dataframe_dict['partition']).agg({'rank':'max'})\n                .withColumnRenamed(f\"max(rank)\", 'rank')\n                .join(\n                    (dataframe.withColumn('rank', row_number().over(Window.partitionBy(dataframe_dict['partition']).orderBy(dataframe_dict['col_rank']))))\n                    ,on = [dataframe_dict['partition'], 'rank']\n                )\n                .drop('rank', 'date')\n            )\n        else:\n            df = self.groupby_function()\n\n        return df\n    \n\n#-------------------------------------------------# DATAFRAMES #-------------------------------------------------#\nimport datetime\nfrom datetime import timedelta\nfrom random import choice, random, randrange\n\ncolumns1 = [\"col_1\",\"col_2\",\"col_4\",\"col_flag\",\"col_date\",\"bool1\",\"bool2\",]\ncolumns2 = [\"col_1\",\"col_2\",\"col_3\",\"jsoncolumn\",\"col_flag2\",\"col_date\",\"bool1\",\"bool2\",]\n\ndata_list1, data_list2 = [], []\n\nstart_date = datetime.datetime.now() - timedelta(days=365)\nend_date = start_date + timedelta(days=365)\n\nfor i in range(1,10):\n    a = (f'label{i}', f'abc{i}', randrange(200), randrange(0,3), (start_date + (end_date - start_date) * random()), choice([True, False]), choice([True, False]))\n    data_list1.append(a)\n    \nfor i in range(1,5):\n    b = (f'label{i}', f'abc{i}', f'def{i}', \"{'col_5':222, 'col_X':'wrong'}\", randrange(0,3), (start_date + (end_date - start_date) * random()), choice([True, False]), choice([True, False]))\n    data_list2.append(b)\n\ndf1 = spark.createDataFrame(data=data_list1,schema=columns1)\ndf2 = spark.createDataFrame(data=data_list2,schema=columns2)\n\n\n#-------------------------------------------------# PARAMETERS (DICTS) #-------------------------------------------------#\n\ndict1 = {\n    'schema':df1,   # or schema\n    'key':'pk',\n    'json_list':[],\n    'columns':{\n        'col_1':'col1',\n        'col_2':'col2',\n        'col_4':'col4',\n        'col_flag':'col_flag',\n        'col_date':'date',\n    },\n    'pk_name' : 'col1_col2',\n    'pk_list':['col1', 'col2'],\n    'fk_name':[],\n    'fk_list':[],\n    'create_flag':['col_flag'],\n    'drop_list':['col2', 'col_flag'],\n    'ranking':True,\n    'partition':'col1_col2',\n    'col_rank':'date',\n}\n\ndict2 = {\n    'schema':df2,   #or schema\n    'key':'fk',\n    'json_list':['jsoncolumn'],\n    'columns':{\n        'col_1':'col1',\n        'col_2':'col2',\n        'col_3':'col3',\n        'jsoncolumn.col_5':'col5',\n        'col_flag2':'col_flag2',\n        'col_date':'date',\n    },\n    'pk_name' : 'col1_col3',\n    'pk_list':['col1', 'col3'],\n    'fk_name':['col1_col2'],\n    'fk_list':[['col1', 'col2']],\n    'create_flag':['col_flag2'],\n    'drop_list':['col2','col3', 'col_flag2,'],\n    'ranking':False,\n    \n}\n#-------------------------------------------------# DICT OF SOURCES DICTS #-------------------------------------------------#\n\ndict_list = {\n    'table1':{\n        'dataframe':'dict1_name',\n        'dict':dict1\n    },\n    'table2':{\n        'dataframe':'dict2_name',\n        'dict':dict2\n    },\n}\n\njoin_key = 'col1_col2'\ncomposed_keys = ['col1', 'col2']\n\n\n#-------------------------------------------------# RUN #-------------------------------------------------#\n\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\n\n\ndfs_dict = {}\n\nfor source in dict_list:\n    df_class = DataPrep(dataframe_dict=dict_list[source]['dict'])\n    dfs_dict[dict_list[source]['dataframe']] = df_class.ranking()\n\nschema = StructType([\n    StructField(join_key, StringType(), True),\n  ])\n\ndf = spark.createDataFrame([], schema)\n\nfor dataframes in dfs_dict.values():\n    df = df.join(dataframes, on=[join_key], how='fullouter')\n\n\n#-------------------------------------------------# DISPLAYS #-------------------------------------------------#\n\ndisplay(dfs_dict['dict1_name'])\ndisplay(dfs_dict['dict2_name'])\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ee927542-f8c8-493a-9e0e-bb2b73486ca5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["label1_abc1","label1",199,1],["label2_abc2","label2",146,1],["label3_abc3","label3",1,0],["label4_abc4","label4",20,1],["label5_abc5","label5",57,1],["label6_abc6","label6",119,0],["label7_abc7","label7",197,1],["label8_abc8","label8",26,1],["label9_abc9","label9",149,0]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"col1_col2","type":"\"string\"","metadata":"{}"},{"name":"col1","type":"\"string\"","metadata":"{}"},{"name":"col4","type":"\"long\"","metadata":"{}"},{"name":"flag_col_flag","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col1_col2</th><th>col1</th><th>col4</th><th>flag_col_flag</th></tr></thead><tbody><tr><td>label1_abc1</td><td>label1</td><td>199</td><td>1</td></tr><tr><td>label2_abc2</td><td>label2</td><td>146</td><td>1</td></tr><tr><td>label3_abc3</td><td>label3</td><td>1</td><td>0</td></tr><tr><td>label4_abc4</td><td>label4</td><td>20</td><td>1</td></tr><tr><td>label5_abc5</td><td>label5</td><td>57</td><td>1</td></tr><tr><td>label6_abc6</td><td>label6</td><td>119</td><td>0</td></tr><tr><td>label7_abc7</td><td>label7</td><td>197</td><td>1</td></tr><tr><td>label8_abc8</td><td>label8</td><td>26</td><td>1</td></tr><tr><td>label9_abc9</td><td>label9</td><td>149</td><td>0</td></tr></tbody></table></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["label1_abc1",222,2,1],["label2_abc2",222,2,1],["label3_abc3",222,1,1],["label4_abc4",222,2,1]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"col1_col2","type":"\"string\"","metadata":"{}"},{"name":"col5","type":"\"long\"","metadata":"{}"},{"name":"col_flag2","type":"\"long\"","metadata":"{}"},{"name":"flag_col_flag2","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col1_col2</th><th>col5</th><th>col_flag2</th><th>flag_col_flag2</th></tr></thead><tbody><tr><td>label1_abc1</td><td>222</td><td>2</td><td>1</td></tr><tr><td>label2_abc2</td><td>222</td><td>2</td><td>1</td></tr><tr><td>label3_abc3</td><td>222</td><td>1</td><td>1</td></tr><tr><td>label4_abc4</td><td>222</td><td>2</td><td>1</td></tr></tbody></table></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["label1_abc1","label1",199,1,222,2,1],["label2_abc2","label2",146,1,222,2,1],["label3_abc3","label3",1,0,222,1,1],["label4_abc4","label4",20,1,222,2,1],["label5_abc5","label5",57,1,null,null,null],["label6_abc6","label6",119,0,null,null,null],["label7_abc7","label7",197,1,null,null,null],["label8_abc8","label8",26,1,null,null,null],["label9_abc9","label9",149,0,null,null,null]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"col1_col2","type":"\"string\"","metadata":"{}"},{"name":"col1","type":"\"string\"","metadata":"{}"},{"name":"col4","type":"\"long\"","metadata":"{}"},{"name":"flag_col_flag","type":"\"integer\"","metadata":"{}"},{"name":"col5","type":"\"long\"","metadata":"{}"},{"name":"col_flag2","type":"\"long\"","metadata":"{}"},{"name":"flag_col_flag2","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col1_col2</th><th>col1</th><th>col4</th><th>flag_col_flag</th><th>col5</th><th>col_flag2</th><th>flag_col_flag2</th></tr></thead><tbody><tr><td>label1_abc1</td><td>label1</td><td>199</td><td>1</td><td>222</td><td>2</td><td>1</td></tr><tr><td>label2_abc2</td><td>label2</td><td>146</td><td>1</td><td>222</td><td>2</td><td>1</td></tr><tr><td>label3_abc3</td><td>label3</td><td>1</td><td>0</td><td>222</td><td>1</td><td>1</td></tr><tr><td>label4_abc4</td><td>label4</td><td>20</td><td>1</td><td>222</td><td>2</td><td>1</td></tr><tr><td>label5_abc5</td><td>label5</td><td>57</td><td>1</td><td>null</td><td>null</td><td>null</td></tr><tr><td>label6_abc6</td><td>label6</td><td>119</td><td>0</td><td>null</td><td>null</td><td>null</td></tr><tr><td>label7_abc7</td><td>label7</td><td>197</td><td>1</td><td>null</td><td>null</td><td>null</td></tr><tr><td>label8_abc8</td><td>label8</td><td>26</td><td>1</td><td>null</td><td>null</td><td>null</td></tr><tr><td>label9_abc9</td><td>label9</td><td>149</td><td>0</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"022c75b5-b0e6-47c5-88d7-87fb8ded0f0c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"dataprep_class","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
